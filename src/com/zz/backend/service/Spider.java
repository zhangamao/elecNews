package com.zz.backend.service;

import java.net.URL;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.http.HttpHost;
import org.apache.http.client.HttpClient;
import org.apache.http.conn.params.ConnRoutePNames;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.protocol.BasicHttpContext;
import org.apache.http.protocol.HttpContext;

import com.zz.backend.model.Article;
import com.zz.backend.service.impl.SpiderForIBM;

public abstract class Spider {

	private static Map<String,Class<? extends Spider>> spiders = new HashMap<String,Class<? extends Spider>>();

	static{
		spiders.put("www.ibm.com", SpiderForIBM.class);
		spiders.put("www.oracle.com", SpiderForIBM.class);
	}

	protected HttpClient httpclient;
	protected String url;
	//存放在解释过程中的状态信息，比如：Article对象列表
	protected HttpContext context; 
	
	//根据URL网址，创建相应的Spider对象
	
	public static Spider getInstance(String url){
		try{
			URL u = new URL(url);
			String host = u.getHost();
			return spiders.get(host).newInstance();
		}catch (Exception e) {
			// TODO: handle exception
			e.printStackTrace();
			throw new RuntimeException("无法找到【"+url+"】对应的爬虫！");
		}
	}
	
	//收集文章
	public List<Article> collect(String url){
		
		//创建HttpClient
		this.httpclient = new DefaultHttpClient();
		this.context = new BasicHttpContext();
		this.url = url;
		
		//设置网络代理
//		httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY,
//				new HttpHost("192.168.1.1", 808));
		
		//执行收集过程
		execute();  //是个抽象方法
		httpclient.getConnectionManager().shutdown();
		
		//获取收集到的文章
		List<Article> articles = (List<Article>)context.getAttribute("articles");
		
		return articles;
	}
	
	public abstract void execute();
	
}
